import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import classification_report
import os
import time

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(page_title="éšæœºæ£®æ—åˆ†ç±» (Random Forest)", page_icon="ğŸŒ²", layout="wide")

st.title("ğŸŒ² éšæœºæ£®æ—åˆ†ç±»ä¸ç½‘æ ¼æœç´¢")
st.markdown("åŸºäº Sentinel-2 æ•°æ®å’Œ NDVI çš„åˆ†ç±»æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°")

st.info("""
**ğŸ’¡ æç¤º / Note**
æœ¬æ¼”ç¤ºè¿è¡Œåœ¨äº‘ç«¯æœåŠ¡å™¨ (2 vCPU, 4GB RAM)ã€‚
ç»æµ‹è¯•ï¼Œ**å…¨å‚æ•°ç½‘æ ¼æœç´¢**å¤§çº¦éœ€è¦ **5 åˆ†é’Ÿ** å³å¯å®Œæˆï¼Œä¸”ç»“æœä¸æœ¬åœ°è®¡ç®—ä¸€è‡´ã€‚
æ‚¨å¯ä»¥æ”¾å¿ƒè¿è¡Œå®Œæ•´æµç¨‹ï¼Œæˆ–ç›´æ¥æŸ¥çœ‹ä¸‹æ–¹â€œ3. è¿è¡Œç»“æœâ€ç« èŠ‚ä¸­çš„é™æ€å±•ç¤ºã€‚
""")

# 1. åŠ è½½æ•°æ®
st.sidebar.header("1. æ•°æ®é…ç½®")
uploaded_file = st.sidebar.file_uploader("ä¸Šä¼  CSV æ–‡ä»¶", type=["csv"])

# å°è¯•åŠ è½½æœ¬åœ°é»˜è®¤æ–‡ä»¶
default_path = 'Data/nanyang_samples.csv'
df = None

if uploaded_file is not None:
    df = pd.read_csv(uploaded_file)
    st.sidebar.success("å·²åŠ è½½ä¸Šä¼ çš„æ–‡ä»¶")
elif os.path.exists(default_path):
    df = pd.read_csv(default_path)
    st.sidebar.info(f"å·²åŠ è½½é»˜è®¤æ–‡ä»¶: {default_path}")
else:
    st.warning(f"è¯·ä¸Šä¼  CSV æ–‡ä»¶æˆ–ç¡®ä¿é¡¹ç›®ç›®å½•ä¸‹å­˜åœ¨ '{default_path}'ã€‚")
    st.stop()

if df is not None:
    with st.expander("æ•°æ®é¢„è§ˆ", expanded=True):
        st.dataframe(df.head())

    # 2. ç‰¹å¾åˆ—è¡¨
    all_columns = df.columns.tolist()
    
    # é»˜è®¤ç‰¹å¾
    default_bands = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B11', 'B12', 'NDVI']
    # æ£€æŸ¥é»˜è®¤ç‰¹å¾æ˜¯å¦éƒ½åœ¨åˆ—ä¸­
    valid_default_bands = [b for b in default_bands if b in all_columns]
    
    st.sidebar.header("2. ç‰¹å¾ä¸æ ‡ç­¾é€‰æ‹©")
    bands = st.sidebar.multiselect("é€‰æ‹©ç‰¹å¾ (Bands)", all_columns, default=valid_default_bands)
    
    # é»˜è®¤æ ‡ç­¾
    default_label = 'class' if 'class' in all_columns else (all_columns[-1] if all_columns else None)
    
    if default_label:
        label_index = all_columns.index(default_label)
    else:
        label_index = 0
        
    label = st.sidebar.selectbox("é€‰æ‹©æ ‡ç­¾ (Label)", all_columns, index=label_index)

    if not bands:
        st.error("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªç‰¹å¾ã€‚")
        st.stop()

    # æ•°æ®é¢„å¤„ç†ï¼šåˆ é™¤é‡‡æ ·ä¸­äº§ç”Ÿçš„ç©ºå€¼
    df_clean = df.dropna(subset=bands + [label])
    X = df_clean[bands]
    y = df_clean[label]
    
    st.sidebar.markdown(f"**æœ‰æ•ˆæ ·æœ¬æ•°:** {len(df_clean)}")

    # 3. åˆ’åˆ†æ•°æ®é›†
    test_size = st.sidebar.slider("æµ‹è¯•é›†æ¯”ä¾‹", 0.1, 0.5, 0.3, 0.05)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

    # 4. é’ˆå¯¹ 10 ä¸ªç‰¹å¾è°ƒæ•´æœç´¢ç½‘æ ¼
    st.sidebar.header("3. ç½‘æ ¼æœç´¢å‚æ•°")
    
    # ä¸ºäº†åœ¨ multiselect ä¸­æ˜¾ç¤º Noneï¼Œæˆ‘ä»¬éœ€è¦å¤„ç†ä¸€ä¸‹
    # n_estimators
    n_estimators_opts = st.sidebar.multiselect("n_estimators", [100, 200, 300, 500], default=[100, 200, 300, 500])
    if not n_estimators_opts: n_estimators_opts = [100]
    
    # max_depth
    # ä½¿ç”¨å­—ç¬¦ä¸² 'None' æ¥ä»£è¡¨ Noneï¼Œç„¶ååœ¨å‚æ•°æ„å»ºæ—¶è½¬æ¢å›å»
    max_depth_options = ['None', 15, 25, 40]
    max_depth_sel = st.sidebar.multiselect("max_depth", max_depth_options, default=['None', 15, 25, 40])
    max_depth_opts = [None if x == 'None' else x for x in max_depth_sel]
    if not max_depth_opts: max_depth_opts = [None]

    # min_samples_split
    min_samples_split_opts = st.sidebar.multiselect("min_samples_split", [2, 5, 10], default=[2, 5, 10])
    if not min_samples_split_opts: min_samples_split_opts = [2]

    # max_features
    max_features_options = ['sqrt', 'log2', 'None']
    max_features_sel = st.sidebar.multiselect("max_features", max_features_options, default=['sqrt', 'log2', 'None'])
    max_features_opts = [None if x == 'None' else x for x in max_features_sel]
    if not max_features_opts: max_features_opts = ['sqrt']

    param_grid = {
        'n_estimators': n_estimators_opts,
        'max_depth': max_depth_opts,
        'min_samples_split': min_samples_split_opts,
        'max_features': max_features_opts
    }
    
    # è®¡ç®—æ€»æ‹Ÿåˆæ¬¡æ•°
    total_combinations = len(n_estimators_opts) * len(max_depth_opts) * len(min_samples_split_opts) * len(max_features_opts)
    total_fits = total_combinations * 5
    
    st.sidebar.markdown("---")
    st.sidebar.info(f"ğŸ“Š å½“å‰é…ç½®:\n- å‚æ•°ç»„åˆæ•°: {total_combinations}\n- æ€»æ‹Ÿåˆæ¬¡æ•° (CV=5): {total_fits}")
    
    if total_fits > 50:
        st.sidebar.warning("âš ï¸ è®­ç»ƒæ¬¡æ•°è¾ƒå¤š (>50)ï¼Œåœ¨ä½é…ç½®æœåŠ¡å™¨ä¸Šå¯èƒ½éœ€è¦æ•°åˆ†é’Ÿï¼Œå»ºè®®å‡å°‘å‚æ•°èŒƒå›´ã€‚")

    if st.button("å¼€å§‹è®­ç»ƒ (Grid Search)", type="primary"):
        start_time = time.time()
        with st.spinner(f'æ­£åœ¨æ‰§è¡Œç½‘æ ¼æœç´¢ (å…± {total_fits} æ¬¡æ‹Ÿåˆ)ï¼Œè¯·ç¨å€™...'):
            # 5. æ‰§è¡Œç½‘æ ¼æœç´¢
            rf = RandomForestClassifier(random_state=42, n_jobs=-1)
            grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', verbose=1)
            grid_search.fit(X_train, y_train)
        
        end_time = time.time()
        elapsed_time = end_time - start_time

        # 6. ç»“æœå±•ç¤º
        st.success(f"âœ… è®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
            
            st.subheader("æœ€ä½³å‚æ•°ä¸ç²¾åº¦")
            col1, col2 = st.columns(2)
            with col1:
                st.write("æœ€ä½³å‚æ•°:")
                st.json(grid_search.best_params_)
            with col2:
                st.metric("äº¤å‰éªŒè¯æœ€é«˜ç²¾åº¦ (OA)", f"{grid_search.best_score_:.4f}")

            # 7. æµ‹è¯•é›†è¯„ä¼°
            best_rf = grid_search.best_estimator_
            y_pred = best_rf.predict(X_test)
            
            st.subheader("æµ‹è¯•é›†åˆ†ç±»æŠ¥å‘Š")
            report_dict = classification_report(y_test, y_pred, output_dict=True)
            st.dataframe(pd.DataFrame(report_dict).transpose().style.format("{:.4f}"))

            # 8. ç‰¹å¾é‡è¦æ€§æ’åºå›¾
            st.subheader("ç‰¹å¾é‡è¦æ€§")
            importances = best_rf.feature_importances_
            indices = np.argsort(importances)[::-1]

            fig, ax = plt.subplots(figsize=(12, 6))
            ax.set_title("Feature Importances (Sentinel-2 + NDVI)")
            sns.barplot(x=[bands[i] for i in indices], y=importances[indices], palette="magma", ax=ax)
            ax.set_ylabel("Importance Score")
            ax.set_xlabel("Bands")
            
            # è‡ªåŠ¨è°ƒæ•´å¸ƒå±€
            plt.tight_layout()
            
            st.pyplot(fig)
